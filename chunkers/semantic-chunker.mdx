---
title: 'SemanticChunker'
description: 'Split text into chunks based on semantic similarity'
icon: 'brain'
---

The `SemanticChunker` splits text into chunks based on semantic similarity, ensuring that related content stays together in the same chunk. This approach is particularly useful for RAG applications where context preservation is crucial.

## Installation

SemanticChunker requires additional dependencies for semantic capabilities. You can install it with:

```bash
pip install "chonkie[semantic]"
```

<Info>For installation instructions, see the [Installation Guide](/getting-started/installation).</Info>

## Initialization

```python
from chonkie import SemanticChunker

# Basic initialization with default parameters
chunker = SemanticChunker(
    embedding_model="minishlab/potion-base-8M",  # Default model
    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or "auto"
    chunk_size=512,                              # Maximum tokens per chunk
    min_sentences=1                              # Initial sentences per chunk
)
```

## Parameters

<ParamField
    path="embedding_model"
    type="Union[str, BaseEmbeddings]"
    default="minishlab/potion-base-8M"
>
    Model identifier or embedding model instance
</ParamField>

<ParamField
    path="mode"
    type="Optional[str]"
    default="window"
>
    Mode for grouping sentences, either "cumulative" or "window"
</ParamField>

<ParamField
    path="threshold"
    type="Optional[float, int, str]"
    default="auto"
>
   When in the range [0,1], denotes the similarity threshold to consider sentences similar.
   When in the range (1,100], interprets the given value as a percentile threshold.
   When set to "auto", the threshold is automatically calculated.
</ParamField>

<ParamField
    path="chunk_size"
    type="int"
    default="512"
>
    Maximum tokens per chunk
</ParamField>

<ParamField
    path="similarity_window"
    type="int"
    default="1"
>
    Number of sentences to consider for similarity threshold calculation
</ParamField>

<ParamField
    path="min_sentences"
    type="int"
    default="1"
>
    Minimum number of sentences per chunk
</ParamField>

<ParamField
    path="min_characters_per_sentence"
    type="int"
    default="12"
>
    Minimum number of characters per sentence
</ParamField>

<ParamField
    path="min_chunk_size"
    type="Optional[int]"
    default="None"
>
    Minimum tokens per chunk
</ParamField>

<ParamField
    path="threshold_step"
    type="Optional[float]"
    default="0.01"
>
    Step size for similarity threshold calculation
</ParamField>

<ParamField
    path="delim"
    type="Union[str, List[str]]"
    default="['.', '!', '?', '\n']"
>
    Delimiters to split sentences on
</ParamField>

## Usage

### Single Text Chunking

```python
text = """First paragraph about a specific topic.
Second paragraph continuing the same topic.
Third paragraph switching to a different topic.
Fourth paragraph expanding on the new topic."""

chunks = chunker.chunk(text)

for chunk in chunks:
    print(f"Chunk text: {chunk.text}")
    print(f"Token count: {chunk.token_count}")
    print(f"Number of sentences: {len(chunk.sentences)}")
```

### Batch Chunking

```python
texts = [
    "First document about topic A...",
    "Second document about topic B..."
]
batch_chunks = chunker.chunk_batch(texts)

for doc_chunks in batch_chunks:
    for chunk in doc_chunks:
        print(f"Chunk: {chunk.text}")
```

## Supported Embeddings

SemanticChunker supports multiple embedding providers through Chonkie's embedding system. See the [Embeddings Overview](/embeddings/overview) for more information.

## Return Type

SemanticChunker returns `SemanticChunk` objects with optimized storage using slots:

```python
@dataclass
class SemanticSentence(Sentence):
    text: str
    start_index: int
    end_index: int
    token_count: int
    embedding: Optional[np.ndarray]  # Sentence embedding vector
    
    __slots__ = ['embedding']  # Optimized memory usage

@dataclass
class SemanticChunk(SentenceChunk):
    text: str
    start_index: int
    end_index: int
    token_count: int
    sentences: List[SemanticSentence]
```